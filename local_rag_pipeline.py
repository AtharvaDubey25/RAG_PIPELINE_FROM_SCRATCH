# -*- coding: utf-8 -*-
"""local rag pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MT_CZLq7D9FKPRyDvGM26-_B4Vw-1YCk
"""

import random
import torch
import numpy as np
import pandas as pd

device = "cuda" if torch.cuda.is_available() else "cpu"

#importing text and embeddings df
text_chunks_and_embeddings_df = pd.read_csv("text_chunks_and_embeddings_df.csv")

#convert embeddings col to np array (got converted to str while converted to csv)
text_chunks_and_embeddings_df["embedding"] = text_chunks_and_embeddings_df["embedding"].apply(lambda x: np.fromstring(x.strip("[]"), sep = " "))

#convert df to list of dicts
pages_and_chunks = text_chunks_and_embeddings_df.to_dict(orient = "records")

#convert embeddings to torch tensor and save to device
embeddings = torch.tensor(np.array(text_chunks_and_embeddings_df["embedding"].tolist()), dtype = torch.float32).to(device)
embeddings.shape

from sentence_transformers import SentenceTransformer, util

embedding_model = SentenceTransformer(model_name_or_path="all-mpnet-base-v2", device=device)

"""### Functionizing the pipeline"""

import time

import textwrap

def print_wrapped(text, wrap_length=80):
    wrapped_text = textwrap.fill(text, wrap_length)
    print(wrapped_text)

def retrieve_relevent_resources(query:str, embeddings:torch.tensor, model: SentenceTransformer=embedding_model, top_k:int=5, print_time:bool = True):

  """embed the query with embedding model and return top k scores and indices from embeddings"""

  #embed the query
  query_embedding = model.encode(query, convert_to_tensor = True)

  #get the dot scores on embeddings
  start_time = time.perf_counter()
  dot_scores = util.dot_score(query_embedding, embeddings)[0]
  end_time = time.perf_counter()

  if print_time:
    print(f"[INFO] time taken to get scores on ({len(embeddings)}) embeddings: {end_time-start_time:.5f} seconds")

  scores, indices = torch.topk(input = dot_scores, k = top_k)

  return scores, indices

def print_top_results(query:str,
                      embeddings: torch.tensor,
                      pages_and_chunks: list[dict]=pages_and_chunks,
                      topk: int = 5):
  scores, indices = retrieve_relevent_resources(query=query, embeddings=embeddings, top_k=topk)
  for score, idx in zip(scores, indices):

    print(f"score: {score:.5f}")
    print("text: \n")
    print_wrapped(f"page: {pages_and_chunks[idx]['sentence_chunk']}")
    print(f"page number: {pages_and_chunks[idx]['page_number']}")
    print("\n")

print_top_results(query = "attention mechanism", embeddings=embeddings)

import torch
gpu_memory_bytes = torch.cuda.get_device_properties(0).total_memory
gpu_memory_gb = round(gpu_memory_bytes / (2**30))
print(f"Available GPU memory: {gpu_memory_gb} GB")

# Note: the following is Gemma focused, however, there are more and more LLMs of the 2B and 7B size appearing for local use.
if gpu_memory_gb < 5.1:
    print(f"Your available GPU memory is {gpu_memory_gb}GB, you may not have enough memory to run a Gemma LLM locally without quantization.")
elif gpu_memory_gb < 8.1:
    print(f"GPU memory: {gpu_memory_gb} | Recommended model: Gemma 2B in 4-bit precision.")
    use_quantization_config = True
    model_id = "google/gemma-2b-it"
elif gpu_memory_gb < 19.0:
    print(f"GPU memory: {gpu_memory_gb} | Recommended model: Gemma 2B in float16 or Gemma 7B in 4-bit precision.")
    use_quantization_config = False
    model_id = "google/gemma-2b-it"
elif gpu_memory_gb > 19.0:
    print(f"GPU memory: {gpu_memory_gb} | Recommend model: Gemma 7B in 4-bit or float16 precision.")
    use_quantization_config = False
    model_id = "google/gemma-7b-it"

print(f"use_quantization_config set to: {use_quantization_config}")
print(f"model_id set to: {model_id}")

from huggingface_hub import login
login()

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers.utils import is_flash_attn_2_available

# 1. Create quantization config for smaller model loading
from transformers import BitsAndBytesConfig
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)

if (is_flash_attn_2_available()) and (torch.cuda.get_device_capability(0)[0] >=8):
  attn_implentation = "flash_attention_2"
else:
  attn_implentation = "sdpa"
print(f"[INFO] Using attention implementation: {attn_implentation}")

model_id = model_id
print(f"[INFO] Using model: {model_id}")

#tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id)

#model
llm_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path = model_id,
                                                 dtype = torch.float16,
                                                 quantization_config = quantization_config if use_quantization_config else None,
                                                 low_cpu_mem_usage = False,
                                                 attn_implementation = attn_implentation)
if not use_quantization_config:
  llm_model.to("cuda")

llm_model

def get_model_num_params(model: torch.nn.Module):
  return sum([params.numel() for params in model.parameters()])

print(f"{get_model_num_params(llm_model)/(10**9)} B ")

def get_model_mem_size(model: torch.nn.Module):

  mem_params = sum([param.nelement() * param.element_size() for param in model.parameters()])
  mem_buffers = sum([buffer.nelement() * buffer.element_size() for buffer in model.buffers()])

  model_mem_bytes = mem_params + mem_buffers # in bytes
  model_mem_mb = model_mem_bytes / (1024**2) # in megabytes
  model_mem_gb = model_mem_bytes / (1024**3) # in gigabytes

  return {"model_mem_bytes": model_mem_bytes,
          "model_mem_mb": round(model_mem_mb, 2),
          "model_mem_gb": round(model_mem_gb, 2)}

get_model_mem_size(llm_model)

"""### Generating text with LLM"""

input_text = "What is Self Attention mechanism?"
print(f"Input text: \n{input_text}")

#prompt template
dialogue_template = [
    {"role":"user",
     "content":input_text
    }
]

#apply the chat template
prompt = tokenizer.apply_chat_template(conversation=dialogue_template, tokenize=False, add_generation_prompt=True)
print(f"\n Prompt:\n{prompt}")

input_ids = tokenizer(prompt, return_tensors="pt").to("cuda")
outputs = tokenizer.decode(llm_model.generate(**input_ids, max_new_tokens = 256)[0])
print(outputs)

query_list = [
    "What is a Large Language Model (LLM), and why is it called 'large'?",
    "What is the difference between traditional machine learning models and LLMs?",
    "What is a transformer architecture, and why is it important for LLMs?",
    "What is the attention mechanism in simple terms?",
    "What is the difference between training, fine-tuning, and inference?",
    "What are embeddings, and why are they used in semantic search?",
    "What is the difference between cosine similarity and dot product?",
    "Why do we chunk documents before creating embeddings in RAG?",
    "What is the difference between an instruction-tuned model and a base model?",
    "What are tokens, and why do they matter in LLMs?"
]

import random
query = random.choice(query_list)

print(f"Query: {query}")
scores, indices = retrieve_relevent_resources(query=query, embeddings=embeddings)
scores, indices

"""### Augementing Prompt with context items"""

def prompt_formatter(query: str, context_items: list[dict])->str:

  context = "- " + "\n- ".join([item["sentence_chunk"] for item in context_items])

  base_prompt = f"""
Answer the question using only the context below.

Context:
{context}

Question:
{query}

Answer:
"""
  dialogue_template = [
      {"role":"user",
      "content":base_prompt}
  ]

  prompt = tokenizer.apply_chat_template(conversation=dialogue_template, tokenize=False, add_generation_prompt=True)
  return prompt

query = random.choice(query_list)

print(f"Query: {query}")
scores, indices = retrieve_relevent_resources(query=query, embeddings=embeddings)

context_items = [pages_and_chunks[i] for i in indices]

prompt = prompt_formatter(query=query, context_items=context_items)
print(prompt)

input_ids = tokenizer(prompt, return_tensors="pt").to("cuda")

outputs = llm_model.generate(**input_ids, max_new_tokens = 256)[0]
output_text = tokenizer.decode(outputs)

print(f"query: {query}")
print(f"rag answer: \n{output_text.replace(prompt,'')}")

